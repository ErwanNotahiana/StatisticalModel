{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa07e6cb",
   "metadata": {},
   "source": [
    "Alohan'ny mamerina dia avereno atao Run ny notebook iray manontolo. Ny fanaovana azy dia red√©marrena mihitsy ny kernel aloha (jereo menubar, safidio **Kernel$\\rightarrow$Restart Kernel and Run All Cells**).\n",
    "\n",
    "Izay misy hoe `YOUR CODE HERE` na \"YOUR ANSWER HERE\" ihany no fenoina. Afaka manampy cells vaovao raha ilaina. Aza adino ny mameno references eo ambany raha ilaina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130d30a5",
   "metadata": {},
   "source": [
    "## References\n",
    "Eto ilay references rehetra no apetraka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be54b306",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/41663874/cs231n-how-to-calculate-gradient-for-softmax-loss-function?fbclid=IwAR2AoT45U6AWM7PCZHPSL1g83o_yoQcwlZPZKd3SjGULm8AAUf6aHtk4NdQ\n",
    "\n",
    "https://github.com/egeyelken/ComputerVisionDL/blob/efc0ee545591c366b815778eb5e0782a60a8577d/eyelken17_61742_assignment1/comp451/classifiers/softmax.py\n",
    "\n",
    "https://github.com/egeyelken/ComputerVisionDL/blob/efc0ee545591c366b815778eb5e0782a60a8577d/eyelken17_61742_assignment1/comp451/classifiers/k_nearest_neighbor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aebdce2a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a003bb8fd03260393cb0121b2dcaf283",
     "grade": false,
     "grade_id": "cell-bb34820fb5daebb2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# AZA MANAMPY CODE ATO FA MNAOVA CELLULE VAOVAO\n",
    "\n",
    "from random import randrange\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.datasets import load_boston, load_diabetes, load_iris, load_digits\n",
    "from scipy.special import huber\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def grad_check_sparse(f, x, analytic_grad, num_checks=12, h=1e-5, error=1e-9):\n",
    "    \"\"\"\n",
    "    sample a few random elements and only return numerical\n",
    "    in this dimensions\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(num_checks):\n",
    "        ix = tuple([randrange(m) for m in x.shape])\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h  # increment by h\n",
    "        fxph = f(x)  # evaluate f(x + h)\n",
    "        x[ix] = oldval - h  # increment by h\n",
    "        fxmh = f(x)  # evaluate f(x - h)\n",
    "        x[ix] = oldval  # reset\n",
    "\n",
    "        grad_numerical = (fxph - fxmh) / (2 * h)\n",
    "        grad_analytic = analytic_grad[ix]\n",
    "        rel_error = abs(grad_numerical - grad_analytic) / (\n",
    "            abs(grad_numerical) + abs(grad_analytic)\n",
    "        )\n",
    "        print(\n",
    "            \"numerical: %f analytic: %f, relative error: %e\"\n",
    "            % (grad_numerical, grad_analytic, rel_error)\n",
    "        )\n",
    "        assert rel_error < error\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72ea596",
   "metadata": {},
   "source": [
    "# Robust linear regression - Huber loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9405baa1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c1ef69a259cc98ed9c1a4b75d4675c8d",
     "grade": false,
     "grade_id": "cell-050726493bce8a41",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erwan/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "data = load_boston()\n",
    "X_train1, y_train1 = data.data, data.target\n",
    "w1 = np.random.randn(X_train1.shape[1]) * 0.0001\n",
    "b1 = np.random.randn(1) * 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b79b3750",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "095fe70b6ce7c4be3afef42fd9756ac5",
     "grade": false,
     "grade_id": "cell-773a9a9798718bb2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def huber_loss_naive(w, b, X, y, epsilon=1.35, alpha=0.0001):\n",
    "    \"\"\"\n",
    "    Huber loss for all observations\n",
    "    \n",
    "    Inputs:\n",
    "    - w: array of shape (D,) containing weights\n",
    "    - b: float bias \n",
    "    - X: array of shape (N, D) containing a minibatch of data\n",
    "    - y: array of shape (N,) containing training labels \n",
    "    - epsilon: float\n",
    "    - alpha: regularization\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dw = np.zeros_like(w)\n",
    "    db = 0.0\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    for i in range (0, n) :\n",
    "        r = (w.T@X[i] +b - y[i])\n",
    "        \n",
    "        if abs(r)<=epsilon :\n",
    "            loss += ((r**2)/2)/n\n",
    "            dw += (X[i] * r)/n\n",
    "            db += r/n\n",
    "        else :\n",
    "            loss += (epsilon * abs(r) - (epsilon**2)/2)/n\n",
    "            dw += (epsilon * np.sign(r) * X[i])/n\n",
    "            db += (epsilon * np.sign(r))/n\n",
    "            \n",
    "    loss += alpha * (np.linalg.norm(w, 2))**2\n",
    "    dw += alpha * 2.0 * w\n",
    "                     \n",
    "    return loss, dw, np.array(db).reshape(1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ca4921",
   "metadata": {},
   "source": [
    "## without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ae434b9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e49c0c5b9c763c872c9eccf3c34e14f",
     "grade": true,
     "grade_id": "cell-88996bb95da97f10",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check w\n",
      "numerical: -24.914970 analytic: -24.914970, relative error: 8.828679e-12\n",
      "numerical: -15.340909 analytic: -15.340909, relative error: 2.129788e-11\n",
      "numerical: -15.340909 analytic: -15.340909, relative error: 2.129788e-11\n",
      "numerical: -551.120158 analytic: -551.120158, relative error: 3.187076e-13\n",
      "numerical: -15.340909 analytic: -15.340909, relative error: 2.129788e-11\n",
      "numerical: -8.484256 analytic: -8.484256, relative error: 1.047315e-11\n",
      "numerical: -12.891700 analytic: -12.891700, relative error: 2.147269e-11\n",
      "numerical: -481.509943 analytic: -481.509943, relative error: 7.998642e-13\n",
      "numerical: -24.914970 analytic: -24.914970, relative error: 8.828679e-12\n",
      "numerical: -481.509943 analytic: -481.509943, relative error: 7.998642e-13\n",
      "numerical: -551.120158 analytic: -551.120158, relative error: 3.187076e-13\n",
      "numerical: -4.878257 analytic: -4.878257, relative error: 3.509817e-10\n",
      "numerical: -24.914970 analytic: -24.914970, relative error: 8.828679e-12\n",
      "numerical: -4.878257 analytic: -4.878257, relative error: 3.509817e-10\n",
      "numerical: -8.484256 analytic: -8.484256, relative error: 1.047315e-11\n",
      "Gradient check bias\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "Gradient check w large epsilon\n",
      "numerical: -90.212689 analytic: -90.212689, relative error: 1.421453e-10\n",
      "numerical: -220.165501 analytic: -220.165501, relative error: 1.414434e-11\n",
      "numerical: -220.165501 analytic: -220.165501, relative error: 1.414434e-11\n",
      "numerical: -50.693635 analytic: -50.693635, relative error: 4.763309e-11\n",
      "numerical: -236.396886 analytic: -236.396886, relative error: 2.857243e-11\n",
      "numerical: -1445.774318 analytic: -1445.774319, relative error: 9.746826e-12\n",
      "numerical: -236.396886 analytic: -236.396886, relative error: 2.857243e-11\n",
      "numerical: -236.396886 analytic: -236.396886, relative error: 2.857243e-11\n",
      "numerical: -90.212689 analytic: -90.212689, relative error: 1.421453e-10\n",
      "numerical: -50.693635 analytic: -50.693635, relative error: 4.763309e-11\n",
      "numerical: -1.964918 analytic: -1.964918, relative error: 5.914770e-10\n",
      "numerical: -405.181383 analytic: -405.181383, relative error: 5.497314e-12\n",
      "numerical: -145.893740 analytic: -145.893740, relative error: 6.722071e-11\n",
      "numerical: -145.893740 analytic: -145.893740, relative error: 6.722071e-11\n",
      "numerical: -8303.927791 analytic: -8303.927791, relative error: 7.109323e-13\n",
      "Gradient check bias large epsilon\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n"
     ]
    }
   ],
   "source": [
    "loss, dw1, db1 = huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=0)\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, dw1, 15, error=1e-9)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, db1, 15, error=1e-9)\n",
    "\n",
    "\n",
    "# Large epsilon\n",
    "large_eps_loss, large_eps_dw1, large_eps_db1 = huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=0)\n",
    "\n",
    "print(\"Gradient check w large epsilon\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, large_eps_dw1, 15, error=1e-9)\n",
    "\n",
    "print(\"Gradient check bias large epsilon\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, large_eps_db1, 15, error=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2b3314",
   "metadata": {},
   "source": [
    " ## with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20ed0b8d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "42c6c09b6a3764a91aa10791fd05545e",
     "grade": true,
     "grade_id": "cell-6cfedcc6f02a9770",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check w\n",
      "numerical: -0.748785 analytic: -0.748785, relative error: 8.990586e-10\n",
      "numerical: -92.576395 analytic: -92.576395, relative error: 1.763455e-12\n",
      "numerical: -15.034591 analytic: -15.034591, relative error: 1.248753e-11\n",
      "numerical: -24.914745 analytic: -24.914745, relative error: 9.714344e-12\n",
      "numerical: -4.878287 analytic: -4.878287, relative error: 3.562240e-10\n",
      "numerical: -24.914745 analytic: -24.914745, relative error: 9.714344e-12\n",
      "numerical: -12.891214 analytic: -12.891214, relative error: 2.348401e-11\n",
      "numerical: -4.878287 analytic: -4.878287, relative error: 3.562240e-10\n",
      "numerical: -15.341111 analytic: -15.341111, relative error: 1.846606e-11\n",
      "numerical: -12.891214 analytic: -12.891214, relative error: 2.348401e-11\n",
      "numerical: -0.748785 analytic: -0.748785, relative error: 8.990586e-10\n",
      "numerical: -17.081830 analytic: -17.081830, relative error: 5.198053e-11\n",
      "numerical: -5.122925 analytic: -5.122925, relative error: 2.357137e-10\n",
      "numerical: -5.122925 analytic: -5.122925, relative error: 2.357137e-10\n",
      "numerical: -481.509710 analytic: -481.509710, relative error: 7.955557e-13\n",
      "Gradient check bias\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "Gradient check w large epsilon\n",
      "numerical: -220.165441 analytic: -220.165441, relative error: 1.464425e-11\n",
      "numerical: -405.181158 analytic: -405.181158, relative error: 4.894135e-12\n",
      "numerical: -1445.774597 analytic: -1445.774597, relative error: 1.003879e-11\n",
      "numerical: -332.809747 analytic: -332.809747, relative error: 7.433067e-12\n",
      "numerical: -8303.927559 analytic: -8303.927559, relative error: 8.069864e-13\n",
      "numerical: -1445.774597 analytic: -1445.774597, relative error: 1.003879e-11\n",
      "numerical: -236.397081 analytic: -236.397081, relative error: 2.742248e-11\n",
      "numerical: -12.027332 analytic: -12.027332, relative error: 7.993090e-12\n",
      "numerical: -50.693665 analytic: -50.693665, relative error: 6.390617e-11\n",
      "numerical: -1.965055 analytic: -1.965055, relative error: 3.381995e-10\n",
      "numerical: -1445.774597 analytic: -1445.774597, relative error: 1.003879e-11\n",
      "numerical: -1.965055 analytic: -1.965055, relative error: 3.381995e-10\n",
      "numerical: -220.165441 analytic: -220.165441, relative error: 1.464425e-11\n",
      "numerical: -1445.774597 analytic: -1445.774597, relative error: 1.003879e-11\n",
      "numerical: -50.693665 analytic: -50.693665, relative error: 6.390617e-11\n",
      "Gradient check bias large epsilon\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n",
      "numerical: -22.501007 analytic: -22.501007, relative error: 2.101917e-10\n"
     ]
    }
   ],
   "source": [
    "loss, dw1, db1 = huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=1)\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, dw1, 15, error=1e-9)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, db1, 15, error=1e-9)\n",
    "\n",
    "\n",
    "# Large epsilon\n",
    "large_eps_loss, large_eps_dw1, large_eps_db1 = huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=1)\n",
    "\n",
    "print(\"Gradient check w large epsilon\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, large_eps_dw1, 15, error=1e-9)\n",
    "\n",
    "print(\"Gradient check bias large epsilon\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, large_eps_db1, 15, error=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "038cf336",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6809cf643812e18004a8e019e8648ee7",
     "grade": false,
     "grade_id": "cell-2f0ecc6405bdde00",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def huber_loss_vectorized(w, b, X, y, epsilon=1.35, alpha=0.0001):\n",
    "    \"\"\"\n",
    "    Huber loss for all observations\n",
    "    \n",
    "    Inputs:\n",
    "    - w: array of shape (D,) containing weights\n",
    "    - b: float bias \n",
    "    - X: array of shape (N, D) containing a minibatch of data\n",
    "    - y: array of shape (N,) containing training labels \n",
    "    - epsilon: float\n",
    "    - alpha: regularization\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dw = np.zeros_like(w)\n",
    "    db = 0\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    r = (X@w +b - y)\n",
    "        \n",
    "    if abs(np.linalg.norm(r, 1))<=epsilon :\n",
    "        loss = ((r**2)/2)/n\n",
    "        dw = (X @ r)/n\n",
    "        db = r/n\n",
    "    else :\n",
    "        loss = (epsilon * abs(np.linalg.norm(r, 1)) - (epsilon**2)/2)/n\n",
    "        dw = (epsilon * np.sign(r) @ X)/n\n",
    "        db = (epsilon * np.sign(r))/n\n",
    "            \n",
    "    loss += alpha * (np.linalg.norm(w, 2))**2\n",
    "    dw += alpha * 2.0 * w   \n",
    "    db = sum(db)\n",
    "    return loss, dw, np.array(db).reshape(1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292f866a",
   "metadata": {},
   "source": [
    "## without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff23bae6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cadca09dc4ee08cf4c73480cfdc14e13",
     "grade": true,
     "grade_id": "cell-cecbe864e0fa149c",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check w\n",
      "numerical: -0.093379 analytic: -0.093379, relative error: 6.023259e-10\n",
      "numerical: -15.340909 analytic: -15.340909, relative error: 7.650619e-12\n",
      "numerical: -17.081635 analytic: -17.081635, relative error: 4.899177e-12\n",
      "numerical: -12.891700 analytic: -12.891700, relative error: 8.039401e-13\n",
      "numerical: -15.340909 analytic: -15.340909, relative error: 7.650619e-12\n",
      "numerical: -0.093379 analytic: -0.093379, relative error: 6.023259e-10\n",
      "numerical: -24.914970 analytic: -24.914970, relative error: 1.255864e-11\n",
      "numerical: -0.093379 analytic: -0.093379, relative error: 6.023259e-10\n",
      "numerical: -0.748838 analytic: -0.748838, relative error: 9.981577e-11\n",
      "numerical: -17.081635 analytic: -17.081635, relative error: 4.899177e-12\n",
      "numerical: -0.748838 analytic: -0.748838, relative error: 9.981577e-11\n",
      "numerical: -551.120158 analytic: -551.120158, relative error: 3.213892e-13\n",
      "numerical: -551.120158 analytic: -551.120158, relative error: 3.213892e-13\n",
      "numerical: -17.081635 analytic: -17.081635, relative error: 4.899177e-12\n",
      "numerical: -92.576117 analytic: -92.576117, relative error: 5.658178e-13\n",
      "Gradient check bias\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "Gradient check w large epsilon\n",
      "numerical: -48150.994269 analytic: -8303.927791, relative error: 7.058209e-01\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15932/275071440.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Check with numerical gradient w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhuber_loss_vectorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m135\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mgrad_numerical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_check_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlarge_eps_dw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gradient check bias large epsilon\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_15932/3000262373.py\u001b[0m in \u001b[0;36mgrad_check_sparse\u001b[0;34m(f, x, analytic_grad, num_checks, h, error)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrad_numerical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_analytic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         )\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mrel_error\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrel_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss, dw1, db1 = huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=0)\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, dw1, 15, error=1e-9)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, db1, 15, error=1e-9)\n",
    "\n",
    "\n",
    "# Large epsilon\n",
    "large_eps_loss, large_eps_dw1, large_eps_db1 = huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=0)\n",
    "\n",
    "print(\"Gradient check w large epsilon\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=135, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, large_eps_dw1, 15, error=1e-9)\n",
    "\n",
    "print(\"Gradient check bias large epsilon\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=135, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, large_eps_db1, 15, error=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d367b1c",
   "metadata": {},
   "source": [
    "## with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee9c47a5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "353c4a3ff31d5ae93f310db8aada6b16",
     "grade": true,
     "grade_id": "cell-fc0f22937553dd38",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check w\n",
      "numerical: -0.748785 analytic: -0.748785, relative error: 4.986978e-11\n",
      "numerical: -24.914745 analytic: -24.914745, relative error: 1.167317e-11\n",
      "numerical: -8.484035 analytic: -8.484035, relative error: 1.945756e-11\n",
      "numerical: -17.081830 analytic: -17.081830, relative error: 5.214421e-12\n",
      "numerical: -0.093516 analytic: -0.093516, relative error: 2.241475e-10\n",
      "numerical: -92.576395 analytic: -92.576395, relative error: 8.028264e-13\n",
      "numerical: -12.891214 analytic: -12.891214, relative error: 2.814482e-12\n",
      "numerical: -15.034591 analytic: -15.034591, relative error: 5.234813e-12\n",
      "numerical: -0.093516 analytic: -0.093516, relative error: 2.241475e-10\n",
      "numerical: -551.120177 analytic: -551.120177, relative error: 3.295374e-13\n",
      "numerical: -15.341111 analytic: -15.341111, relative error: 1.048206e-11\n",
      "numerical: -8.484035 analytic: -8.484035, relative error: 1.945756e-11\n",
      "numerical: -92.576395 analytic: -92.576395, relative error: 8.028264e-13\n",
      "numerical: -15.341111 analytic: -15.341111, relative error: 1.048206e-11\n",
      "numerical: -15.341111 analytic: -15.341111, relative error: 1.048206e-11\n",
      "Gradient check bias\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "numerical: -1.350000 analytic: -1.350000, relative error: 7.813972e-11\n",
      "Gradient check w large epsilon\n",
      "numerical: -2491.496810 analytic: -405.181158, relative error: 7.202443e-01\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15932/979431413.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Check with numerical gradient w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhuber_loss_vectorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m135\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mgrad_numerical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_check_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlarge_eps_dw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gradient check bias large epsilon\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_15932/3000262373.py\u001b[0m in \u001b[0;36mgrad_check_sparse\u001b[0;34m(f, x, analytic_grad, num_checks, h, error)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrad_numerical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_analytic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         )\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mrel_error\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrel_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss, dw1, db1 = huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=1)\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, dw1, 15, error=1e-9)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, db1, 15, error=1e-9)\n",
    "\n",
    "\n",
    "# Large epsilon\n",
    "large_eps_loss, large_eps_dw1, large_eps_db1 = huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=1)\n",
    "\n",
    "print(\"Gradient check w large epsilon\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=135, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, large_eps_dw1, 15, error=1e-9)\n",
    "\n",
    "print(\"Gradient check bias large epsilon\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=135, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, large_eps_db1, 15, error=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed260f95",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51a2209d0893d2b94c64bc684159f81c",
     "grade": false,
     "grade_id": "cell-9bc0bf420f2797ba",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LinearModel():\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def train(self, X, y, learning_rate=1e-3, alpha=0.0001, num_iters=100, batch_size=200, verbose=False):\n",
    "        N, d = X.shape\n",
    "        \n",
    "        if self.w is None: # Initialization\n",
    "            self.w = 0.001 * np.random.randn(d)\n",
    "            self.b = 0.0\n",
    "\n",
    "        # Run stochastic gradient descent to optimize w\n",
    "        \n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "                                                               \n",
    "            # Sample batch_size elements in X_batch and y_batch\n",
    "            # X_batch shape is  (batch_size, d) and y_batch shape is (batch_size,)                                                                                          \n",
    "            # Hint: Use np.random.choice to generate indices\n",
    "            ## echantillonage\n",
    "            i = np.random.choice (X.shape[0], batch_size)\n",
    "            ## building matrix X and vector y\n",
    "            X_batch = X[i, :]\n",
    "            y_batch = y[i]\n",
    "            \n",
    "            # evaluate loss and gradient\n",
    "            loss, dw, db = self.loss(X_batch, y_batch, alpha)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            # perform parameter update                                                                \n",
    "            # Update the weights w and bias b using the gradient and the learning rate.          \n",
    "            self.w = self.w - learning_rate * dw\n",
    "            self.b = self.b - learning_rate * db\n",
    "            \n",
    "            if verbose and it % 10000 == 0:\n",
    "                print(\"iteration %d / %d: loss %f\" % (it, num_iters, loss))\n",
    "                \n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        pass\n",
    "\n",
    "class HuberRegression(LinearModel):\n",
    "    \"\"\" Linear regression \"\"\"\n",
    "\n",
    "    def loss(self, X_batch, y_batch, alpha):\n",
    "        return huber_loss_vectorized(self.w, self.b, X_batch, y_batch, alpha=alpha)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X@self.w + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "766c7c05",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0baa5b2cf387331bd0a892f09b68ffc",
     "grade": true,
     "grade_id": "cell-43a5575eb3552466",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 75000: loss 32.174303\n",
      "iteration 10000 / 75000: loss 3.892807\n",
      "iteration 20000 / 75000: loss 5.228802\n",
      "iteration 30000 / 75000: loss 4.275602\n",
      "iteration 40000 / 75000: loss 5.417389\n",
      "iteration 50000 / 75000: loss 3.946746\n",
      "iteration 60000 / 75000: loss 3.263848\n",
      "iteration 70000 / 75000: loss 3.260043\n",
      "MSE scikit-learn: 24.04102301055777\n",
      "MSE gradient descent model : 24.681378410394075\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train1 = scaler.fit_transform(X_train1)\n",
    "\n",
    "sk_model = HuberRegressor(fit_intercept=True)\n",
    "sk_model.fit(X_train1, y_train1)\n",
    "sk_pred = sk_model.predict(X_train1)\n",
    "sk_mse = mean_squared_error(sk_pred, y_train1)\n",
    "\n",
    "model = HuberRegression()\n",
    "model.train(X_train1, y_train1, num_iters=75000, batch_size=64, learning_rate=1e-2, verbose=True)\n",
    "pred = model.predict(X_train1)\n",
    "mse = mean_squared_error(pred, y_train1)\n",
    "\n",
    "print(\"MSE scikit-learn:\", sk_mse)\n",
    "print(\"MSE gradient descent model :\", mse)\n",
    "assert mse - sk_mse < 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f24bf4",
   "metadata": {},
   "source": [
    "# Multinomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b194affb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fcf6c9bab4975732f03ce6c8215e72be",
     "grade": false,
     "grade_id": "cell-27c3352e3785ecdb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X_train2, y_train2 = data.data, data.target\n",
    "\n",
    "W = np.random.randn(X_train2.shape[1], 3) * 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8405c63",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af6fecc0ab19bd2c392db4255a946d03",
     "grade": false,
     "grade_id": "cell-9049b2a8d3edaeaf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax_loss_naive(W, X, y, alpha):\n",
    "    \"\"\"\n",
    "    Softmax loss function WITH FOR LOOPS\n",
    "\n",
    "    Inputs:\n",
    "    - W: array of shape (D, C) containing weights\n",
    "    - X: array of shape (N, D) containing a minibatch of data\n",
    "    - y: array of shape (N,) containing training labels\n",
    "    - alpha: (float) regularization \n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss as single float\n",
    "    - gradient with respect to weights W;  same shape as W\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialization\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "\n",
    "    \n",
    "    # Tandremo ny numeric instability\n",
    "\n",
    "    # Get shapes\n",
    "    classes = W.shape[1]\n",
    "    n = X.shape[0]\n",
    "\n",
    "    Xt = np.transpose(X)\n",
    "    Wt = np.transpose(W)\n",
    "    \n",
    "    dW = dW.T\n",
    "    \n",
    "    for i in range(0, n):\n",
    "        # Compute vector of scores\n",
    "        f_i = Wt.dot(Xt[:, i]) # in R^{num_classes}\n",
    "        #print(f_i)\n",
    "\n",
    "        # Normalization trick to avoid numerical instability, per http://cs231n.github.io/linear-classify/#softmax\n",
    "        log_c = np.max(f_i)\n",
    "        f_i -= log_c\n",
    "\n",
    "        # Compute loss (and add to it, divided later)\n",
    "        # L_i = - f(x_i)_{y_i} + log \\sum_j e^{f(x_i)_j}\n",
    "        sum_i = 0.0\n",
    "        for f_i_j in f_i:\n",
    "            sum_i += np.exp(f_i_j)\n",
    "        loss += -f_i[y[i]] + np.log(sum_i)\n",
    "\n",
    "        # Compute gradient\n",
    "        # dw_j = 1/num_train * \\sum_i[x_i * (p(y_i = j)-Ind{y_i = j} )]\n",
    "        # Here we are computing the contribution to the inner sum for a given i.\n",
    "        \n",
    "        for j in range(classes):\n",
    "            p = np.exp(f_i[j])/sum_i\n",
    "            dW[j, :] += (p - (j == y[i])) * Xt[:, i]\n",
    "\n",
    "    # Compute average\n",
    "    loss /= n\n",
    "    dW /= n\n",
    "\n",
    "  # Regularization\n",
    "    loss += 0.5 * alpha * np.sum(Wt * Wt)\n",
    "    dW += alpha*Wt\n",
    "    \n",
    "    dW = dW.T\n",
    "    \n",
    "    \"\"\"\n",
    "    for k in range(0, n):\n",
    "        scores_diff = np.dot(X, W)[k] - np.max(np.dot(X, W)[k])\n",
    "        softmax = np.exp(scores_diff) / np.sum(np.exp(scores_diff))\n",
    "        loss = loss - np.log(softmax[y[k]])\n",
    "        dW[:,y[k]] -= X[k]\n",
    "        for i in range(W.shape[1]):\n",
    "            dW[:, i] += softmax[i] * X[k]\n",
    "        loss /= X.shape[0]\n",
    "        dW /= X.shape[0]\n",
    "    \"\"\"        \n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9aa529",
   "metadata": {},
   "source": [
    "## Without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0eea225f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac16981288c3c2f604ad8bab7defad33",
     "grade": true,
     "grade_id": "cell-878cebbdfa4e3f9b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -0.166913 analytic: -0.166913, relative error: 2.213164e-10\n",
      "numerical: 0.027797 analytic: 0.027797, relative error: 2.903960e-10\n",
      "numerical: -0.166913 analytic: -0.166913, relative error: 2.213164e-10\n",
      "numerical: 0.278622 analytic: 0.278622, relative error: 3.335686e-10\n",
      "numerical: 0.317666 analytic: 0.317666, relative error: 3.153628e-11\n",
      "numerical: 0.027797 analytic: 0.027797, relative error: 2.903960e-10\n",
      "numerical: 0.278622 analytic: 0.278622, relative error: 3.335686e-10\n",
      "numerical: -0.275592 analytic: -0.275592, relative error: 8.172165e-11\n",
      "numerical: -0.123797 analytic: -0.123797, relative error: 3.115548e-10\n",
      "numerical: -0.275592 analytic: -0.275592, relative error: 8.172165e-11\n",
      "numerical: -0.030384 analytic: -0.030384, relative error: 4.221851e-09\n",
      "numerical: -0.166913 analytic: -0.166913, relative error: 2.213164e-10\n"
     ]
    }
   ],
   "source": [
    "loss, dW = softmax_loss_naive(W, X_train2, y_train2, 0.0)\n",
    "\n",
    "f = lambda W: softmax_loss_naive(W, X_train2, y_train2, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, dW, error=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edea2f2",
   "metadata": {},
   "source": [
    "## With regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "533001ff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd9a1e462d23666f55369d6b9d152117",
     "grade": true,
     "grade_id": "cell-ead29b6569263a0f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -0.123468 analytic: -0.123468, relative error: 2.880854e-10\n",
      "numerical: 0.765254 analytic: 0.765254, relative error: 7.183249e-11\n",
      "numerical: 0.317544 analytic: 0.317544, relative error: 2.566049e-11\n",
      "numerical: -0.275895 analytic: -0.275895, relative error: 8.179865e-11\n",
      "numerical: -0.598010 analytic: -0.598010, relative error: 8.200295e-11\n",
      "numerical: 0.095980 analytic: 0.095980, relative error: 3.987252e-10\n",
      "numerical: -0.275895 analytic: -0.275895, relative error: 8.179865e-11\n",
      "numerical: -0.030006 analytic: -0.030006, relative error: 4.299489e-09\n",
      "numerical: -0.275895 analytic: -0.275895, relative error: 8.179865e-11\n",
      "numerical: 0.765254 analytic: 0.765254, relative error: 7.183249e-11\n",
      "numerical: -0.275895 analytic: -0.275895, relative error: 8.179865e-11\n",
      "numerical: 0.317544 analytic: 0.317544, relative error: 2.566049e-11\n"
     ]
    }
   ],
   "source": [
    "loss, dW = softmax_loss_naive(W, X_train2, y_train2, 2)\n",
    "\n",
    "f = lambda W: softmax_loss_naive(W, X_train2, y_train2, 2)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, dW, error=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2593d82",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48f52a4942e2b468f5775c5c7fbd2617",
     "grade": false,
     "grade_id": "cell-11e6980597b20334",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax_loss_vectorized(W, X, y, alpha, fit_intercept=False):\n",
    "    \"\"\"\n",
    "    Softmax loss function WITHOUT FOR LOOPS\n",
    "\n",
    "    Inputs:\n",
    "    - W: array of shape (D, C) containing weights\n",
    "    - X: array of shape (N, D) containing a minibatch of data\n",
    "    - y: array of shape (N,) containing training labels\n",
    "    - alpha: (float) regularization \n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss as single float\n",
    "    - gradient with respect to weights W;  same shape as W\n",
    "    \"\"\"\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "\n",
    "    if fit_intercept :\n",
    "        ## inserting a column of 1 in the features matrix\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    \n",
    "    wx = np.dot(X, W) - np.max(np.dot(X, W), axis=1, keepdims=True)\n",
    "\n",
    "    softmax = np.exp(wx)/np.sum(np.exp(wx), axis=1,keepdims=True)\n",
    "    loss = np.sum(-np.log(softmax[np.arange(n), y]))\n",
    "    softmax[np.arange(n), y] -= 1\n",
    "\n",
    "    dW = np.dot(np.transpose(X), softmax)\n",
    "    loss /= n\n",
    "    dW /= n\n",
    "\n",
    "    loss += alpha * np.sum(W*W)\n",
    "    dW += alpha * 2 * W\n",
    "\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddea508b",
   "metadata": {},
   "source": [
    "## Without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78f759e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "441d40ba642d3a505582eaeb87981127",
     "grade": true,
     "grade_id": "cell-0bc0fcf0bd3d3f13",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 0.027797 analytic: 0.027797, relative error: 4.900945e-10\n",
      "numerical: -0.166913 analytic: -0.166913, relative error: 3.543458e-10\n",
      "numerical: -0.166913 analytic: -0.166913, relative error: 3.543458e-10\n",
      "numerical: 0.317666 analytic: 0.317666, relative error: 1.406152e-11\n",
      "numerical: 0.278622 analytic: 0.278622, relative error: 4.730311e-10\n",
      "numerical: -0.123797 analytic: -0.123797, relative error: 1.770323e-10\n",
      "numerical: -0.248239 analytic: -0.248239, relative error: 5.176673e-10\n",
      "numerical: 0.764994 analytic: 0.764994, relative error: 5.461293e-11\n",
      "numerical: 0.278622 analytic: 0.278622, relative error: 4.730311e-10\n",
      "numerical: -0.275592 analytic: -0.275592, relative error: 1.899073e-11\n",
      "numerical: 0.278622 analytic: 0.278622, relative error: 4.730311e-10\n",
      "numerical: 0.278622 analytic: 0.278622, relative error: 4.730311e-10\n"
     ]
    }
   ],
   "source": [
    "loss, dW = softmax_loss_vectorized(W, X_train2, y_train2, 0.0)\n",
    "\n",
    "f = lambda W: softmax_loss_vectorized(W, X_train2, y_train2, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, dW, error=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c84f53",
   "metadata": {},
   "source": [
    "## With regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bddf43a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6475ed3a14441d5488ecb46ec2522bbb",
     "grade": true,
     "grade_id": "cell-1fe0149d053dade0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 0.028105 analytic: 0.028105, relative error: 6.355398e-10\n",
      "numerical: 0.317422 analytic: 0.317422, relative error: 1.978019e-11\n",
      "numerical: -0.041365 analytic: -0.041365, relative error: 2.559992e-10\n",
      "numerical: -0.276197 analytic: -0.276197, relative error: 1.861630e-11\n",
      "numerical: -0.247341 analytic: -0.247341, relative error: 5.078883e-10\n",
      "numerical: -0.276197 analytic: -0.276197, relative error: 1.861630e-11\n",
      "numerical: -0.166446 analytic: -0.166446, relative error: 3.622007e-10\n",
      "numerical: 0.317422 analytic: 0.317422, relative error: 1.978019e-11\n",
      "numerical: 0.317422 analytic: 0.317422, relative error: 1.978019e-11\n",
      "numerical: 0.278848 analytic: 0.278848, relative error: 4.733501e-10\n",
      "numerical: -0.041365 analytic: -0.041365, relative error: 2.559992e-10\n",
      "numerical: -0.247341 analytic: -0.247341, relative error: 5.078883e-10\n"
     ]
    }
   ],
   "source": [
    "loss, dW = softmax_loss_vectorized(W, X_train2, y_train2, 2)\n",
    "\n",
    "f = lambda W: softmax_loss_vectorized(W, X_train2, y_train2, 2)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, dW, error=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7e247a",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9dff0bbc",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0cedf5c70f5cf04cdd8b74702815dba",
     "grade": false,
     "grade_id": "cell-0f69bb891603b665",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LinearModel():\n",
    "    def __init__(self, fit_intercept=True):\n",
    "        self.W = None\n",
    "        self.fit_intercept = fit_intercept\n",
    "\n",
    "    def train(self, X, y, learning_rate=1e-3, alpha=0, num_iters=100, batch_size=200, verbose=False):\n",
    "        if self.fit_intercept:\n",
    "            ## inserting a column of 1 in the features matrix\n",
    "            X = np.insert(X, 0, 1, axis=1)\n",
    "            \n",
    "        N, d = X.shape\n",
    "        \n",
    "        C = (np.max(y) + 1) \n",
    "        if self.W is None: # Initialization\n",
    "            self.W = 0.001 * np.random.randn(d, C)\n",
    "\n",
    "        # Run stochastic gradient descent to optimize W\n",
    "        \n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "                                                               \n",
    "            # Sample batch_size elements in X_batch and y_batch\n",
    "            # X_batch shape is  (batch_size, d) and y_batch shape is (batch_size,)                                                                                          \n",
    "            # Hint: Use np.random.choice to generate indices\n",
    "            i = np.random.choice (X.shape[0], batch_size)\n",
    "            ## building matrix X and vector y\n",
    "            X_batch = X[i, :]\n",
    "            y_batch = y[i]\n",
    "            \n",
    "            # evaluate loss and gradient\n",
    "            loss, dW = self.loss(X_batch, y_batch, alpha)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            # perform parameter update                                                                \n",
    "            # Update the weights w using the gradient and the learning rate.          \n",
    "            self.W = self.W - learning_rate * dW\n",
    "            \n",
    "            if verbose and it % 10000 == 0:\n",
    "                print(\"iteration %d / %d: loss %f\" % (it, num_iters, loss))\n",
    "                \n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        pass\n",
    "\n",
    "class MultinomialLogisticRegressor(LinearModel):\n",
    "    \"\"\" Softmax regression \"\"\"\n",
    "\n",
    "    def loss(self, X_batch, y_batch, alpha):\n",
    "        return softmax_loss_vectorized(self.W, X_batch, y_batch, alpha)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" \n",
    "        Inputs:\n",
    "        - X: array of shape (N, D) \n",
    "\n",
    "        Returns:\n",
    "        - y_pred: 1-dimensional array of length N, each element is an integer giving the predicted class \n",
    "        \"\"\"\n",
    "        if self.fit_intercept:\n",
    "           ## inserting a column of 1 in the features matrix\n",
    "           X = np.insert(X, 0, 1, axis=1)\n",
    "            \n",
    "        y_pred = np.argmax(np.dot(X, self.W), axis = 1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce21aaa4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56aacc6a9e4ede50cd934a81dfd9915f",
     "grade": true,
     "grade_id": "cell-8569aecb5759a819",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 75000: loss 1.098873\n",
      "iteration 10000 / 75000: loss 0.443696\n",
      "iteration 20000 / 75000: loss 0.329909\n",
      "iteration 30000 / 75000: loss 0.240563\n",
      "iteration 40000 / 75000: loss 0.336661\n",
      "iteration 50000 / 75000: loss 0.336973\n",
      "iteration 60000 / 75000: loss 0.434587\n",
      "iteration 70000 / 75000: loss 0.428090\n",
      "Accuracy scikit-learn: 0.86\n",
      "Accuracy gradient descent model : 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train2 = scaler.fit_transform(X_train2)\n",
    "\n",
    "sk_model = LogisticRegression(fit_intercept=False)\n",
    "sk_model.fit(X_train2, y_train2)\n",
    "sk_pred = sk_model.predict(X_train2)\n",
    "sk_accuracy = accuracy_score(y_train2, sk_pred)\n",
    "\n",
    "model = MultinomialLogisticRegressor(fit_intercept=False)\n",
    "model.train(X_train2, y_train2, num_iters=75000, batch_size=64, learning_rate=1e-3, verbose=True)\n",
    "pred = model.predict(X_train2)\n",
    "model_accuracy = accuracy_score(y_train2, pred)\n",
    "\n",
    "print(\"Accuracy scikit-learn:\", sk_accuracy)\n",
    "print(\"Accuracy gradient descent model :\", model_accuracy)\n",
    "assert sk_accuracy - model_accuracy < 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2c6a8ad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40a4732103eaf77860c941e0897de01c",
     "grade": true,
     "grade_id": "cell-30e12569fdfb269c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 75000: loss 1.096789\n",
      "iteration 10000 / 75000: loss 0.282968\n",
      "iteration 20000 / 75000: loss 0.221421\n",
      "iteration 30000 / 75000: loss 0.254042\n",
      "iteration 40000 / 75000: loss 0.152881\n",
      "iteration 50000 / 75000: loss 0.204131\n",
      "iteration 60000 / 75000: loss 0.137046\n",
      "iteration 70000 / 75000: loss 0.132937\n",
      "Accuracy scikit-learn: 0.9733333333333334\n",
      "Accuracy gradient descent model : 0.96\n"
     ]
    }
   ],
   "source": [
    "sk_model = LogisticRegression(fit_intercept=True)\n",
    "sk_model.fit(X_train2, y_train2)\n",
    "sk_pred = sk_model.predict(X_train2)\n",
    "sk_accuracy = accuracy_score(y_train2, sk_pred)\n",
    "\n",
    "model = MultinomialLogisticRegressor(fit_intercept=True)\n",
    "model.train(X_train2, y_train2, num_iters=75000, batch_size=64, learning_rate=1e-3, verbose=True)\n",
    "pred = model.predict(X_train2)\n",
    "model_accuracy = accuracy_score(y_train2, pred)\n",
    "\n",
    "print(\"Accuracy scikit-learn:\", sk_accuracy)\n",
    "print(\"Accuracy gradient descent model :\", model_accuracy)\n",
    "assert sk_accuracy - model_accuracy < 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62032646",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c81034",
   "metadata": {},
   "source": [
    "## Computing distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a97c549",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "858935a2455a3ca0e416b3084b730e7f",
     "grade": false,
     "grade_id": "cell-9738d380318b956f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "data = load_digits()\n",
    "X_train3, y_train3 = data.data, data.target\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X_train3, y_train3, test_size=0.33, random_state=2)\n",
    "\n",
    "def get_distances_two_loops_with_norm(X_train, X_test):\n",
    "    num_test = X_test.shape[0]\n",
    "    num_train = X_train.shape[0]\n",
    "    distances = np.zeros((num_test, num_train))\n",
    "    for i in range(num_test):\n",
    "        for j in range(num_train):\n",
    "            distances[i, j] = np.linalg.norm(X_test[i] - X_train[j])\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1b01336",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6cfd476a7819d38b3c8ef47443365f64",
     "grade": false,
     "grade_id": "cell-4756294c792f6985",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_distances_two_loops(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Compute the distance between each test point in X_test and each training point\n",
    "    in X_train \n",
    "\n",
    "    Inputs:\n",
    "    - X_test: array of shape (num_test, D) \n",
    "\n",
    "    Returns:\n",
    "    - distances: array of shape (num_test, num_train), dists[i, j] is Euclidean distance between \n",
    "    the ith test point and the jth training point.\n",
    "    \"\"\"\n",
    "    num_test = X_test.shape[0]\n",
    "    num_train = X_train.shape[0]\n",
    "    distances = np.zeros((num_test, num_train))\n",
    "    for i in range(num_test):\n",
    "        for j in range(num_train):\n",
    "            # Ataovy ao anaty distances[i, j] ny distance entre ith test point sy th training point\n",
    "            # Aza manao boucle instony ato anatiny\n",
    "            # TSY MAHAZO MAMPIASA np.linalg.norm() :D\n",
    "            distances[i][j] = np.sqrt(np.sum((X_test[i]-X_train[j])**2))\n",
    "                                      \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da674674",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78d07191456013c3a22619c2d26e2503",
     "grade": true,
     "grade_id": "cell-76b63cae5d6a5977",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "distances = get_distances_two_loops(X_train3, X_test3)\n",
    "true_distances = get_distances_two_loops_with_norm(X_train3, X_test3)\n",
    "\n",
    "difference = np.linalg.norm(distances - true_distances, ord='fro')\n",
    "\n",
    "print(difference)\n",
    "assert difference < 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69a0a04b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b98a31f8ab9e6b8bd5a18ea02dd7165",
     "grade": false,
     "grade_id": "cell-05658431df004915",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_distances_one_loop(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Compute the distance between each test point in X_test and each training point\n",
    "    in X_train \n",
    "\n",
    "    Inputs:\n",
    "    - X_test: array of shape (num_test, D) \n",
    "\n",
    "    Returns:\n",
    "    - dists: array of shape (num_test, num_train), dists[i, j] is Euclidean distance between \n",
    "    the ith test point and the jth training point.\n",
    "    \"\"\"\n",
    "    num_test = X_test.shape[0]\n",
    "    num_train = X_train.shape[0]\n",
    "    distances = np.zeros((num_test, num_train))\n",
    "    for i in range(num_test):\n",
    "        # Ataovy ao anaty dists[i, j] ny distance entre ith test point sy th training point\n",
    "        # Aza manao boucle instony ato anatiny\n",
    "        # TSY MAHAZO MAMPIASA np.linalg.norm() :D\n",
    "        distances[i] = np.sqrt(np.sum((X_test[i] - X_train)**2, axis=1))\n",
    "        \n",
    "    return distances    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2540b717",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a9d09964868df2c201e48372082c2e5",
     "grade": true,
     "grade_id": "cell-cfa64f63da5511f9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "distances = compute_distances_one_loop(X_train3, X_test3)\n",
    "true_distances = get_distances_two_loops_with_norm(X_train3, X_test3)\n",
    "\n",
    "difference = np.linalg.norm(distances - true_distances, ord='fro')\n",
    "\n",
    "print(difference)\n",
    "assert difference < 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3788be0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e10c370f1f9c3e653fc817c883b7de0e",
     "grade": false,
     "grade_id": "cell-3cb10928a6af9889",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_distances_zero_loop(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Compute the distance between each test point in X_test and each training point\n",
    "    in X_train \n",
    "\n",
    "    Inputs:\n",
    "    - X_test: array of shape (num_test, D) \n",
    "\n",
    "    Returns:\n",
    "    - distances: array of shape (num_test, num_train), dists[i, j] is Euclidean distance between \n",
    "    the ith test point and the jth training point.\n",
    "    \"\"\"\n",
    "    num_test = X_test.shape[0]\n",
    "    num_train = X_train.shape[0]\n",
    "    distances = np.zeros((num_test, num_train))  \n",
    "    # Ataovy ao anaty dists[i, j] ny distance entre ith test point sy th training point\n",
    "    # Aza manao boucle instony\n",
    "    # TSY MAHAZO MAMPIASA np.linalg.norm() NA FONCTIONS AO AMIN'NY SCIPY :D\n",
    "            \n",
    "\n",
    "    distances = np.sqrt(np.sum(X_test**2, axis = 1)[:, np.newaxis] + np.sum(X_train**2, axis = 1) - 2 * (np.dot(X_test, np.transpose(X_train))))\n",
    "    \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81a9294b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08a63c2a5bf229c55db5bdc793f5c885",
     "grade": true,
     "grade_id": "cell-ff999d4dddc0760f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "distances = get_distances_zero_loop(X_train3, X_test3)\n",
    "true_distances = get_distances_two_loops_with_norm(X_train3, X_test3)\n",
    "\n",
    "difference = np.linalg.norm(distances - true_distances, ord='fro')\n",
    "\n",
    "print(difference)\n",
    "assert difference < 1e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9c2ac6",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbor (knn) classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ce1e0f2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2213945856056edb1cbc5174c98be4e",
     "grade": false,
     "grade_id": "cell-8d852d04f1f1e1c6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class KNearestNeighborClassifier():\n",
    "    \"\"\" kNN classifier using L2 distance \"\"\"\n",
    "\n",
    "    def __init__(self, k=1):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - k: number of nearest neighbors that vote for the predicted labels.\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the classifier. Just memorize the training data.\n",
    "\n",
    "        Inputs:\n",
    "        - X: array of shape (num_train, D) \n",
    "        - y: array of shape (N,) \n",
    "        \"\"\"\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict labels for test data using this classifier.\n",
    "\n",
    "        Inputs:\n",
    "        - X: array of shape (num_test, D) \n",
    "\n",
    "        Returns:\n",
    "        - y: array of shape (num_test,) \n",
    "        \"\"\"\n",
    "        distances = get_distances_zero_loop(self.X_train, X)\n",
    "        return self.predict_labels(distances)\n",
    "\n",
    "    def predict_labels(self, distances):\n",
    "        \"\"\"\n",
    "        Given a matrix of distances between test points and training points,\n",
    "        predict a label for each test point.\n",
    "\n",
    "        Inputs:\n",
    "        - distances: array of shape (num_test, num_train), dists[i, j] is Euclidean distance between \n",
    "        the ith test point and the jth training point.\n",
    "\n",
    "        Returns:\n",
    "        - y:  array of shape (num_test,) \n",
    "        \"\"\"\n",
    "        num_test = distances.shape[0]\n",
    "        y_pred = np.zeros(num_test)\n",
    "        for i in range(num_test):\n",
    "            # list storing the labels of the k nearest neighbors to the ith test point.\n",
    "            closest_y = []\n",
    "\n",
    "            # Ampidirina ao anaty closest_y ny labels an'ny k neighbors akaiky indrindra\n",
    "            # Jereo fampiasana np.argsort\n",
    "            closest_y = self.y_train[np.argsort(distances[i])][0:self.k]\n",
    "\n",
    "            # Tadiavo ny label betsaka indrindra dia iny no atao prediction\n",
    "            # Raha misy mitovy dia izay label kely raisina\n",
    "            y_pred[i] = np.bincount(closest_y).argmax()\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c419c1ac",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5ccea2db11ef9d1310203f6b2227fbc",
     "grade": true,
     "grade_id": "cell-301db44591c60d17",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erwan/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scikit-learn: 0.9831649831649831\n",
      "Accuracy gradient descent model : 0.9831649831649831\n"
     ]
    }
   ],
   "source": [
    "sk_model = KNeighborsClassifier(n_neighbors=3)\n",
    "sk_model.fit(X_train3, y_train3)\n",
    "sk_pred = sk_model.predict(X_test3)\n",
    "sk_accuracy = accuracy_score(y_test3, sk_pred)\n",
    "\n",
    "model = KNearestNeighborClassifier(k=3)\n",
    "model.fit(X_train3, y_train3)\n",
    "pred = model.predict(X_test3)\n",
    "model_accuracy = accuracy_score(y_test3, pred)\n",
    "\n",
    "print(\"Accuracy scikit-learn:\", sk_accuracy)\n",
    "print(\"Accuracy gradient descent model :\", model_accuracy)\n",
    "assert sk_accuracy - model_accuracy < 1e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792bbc93",
   "metadata": {},
   "source": [
    "## cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4c44832",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0433a47c347be926b4625f181c01d120",
     "grade": false,
     "grade_id": "cell-adaaf6c0d8cdce0a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n",
      "/home/erwan/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n"
     ]
    }
   ],
   "source": [
    "num_folds = 5\n",
    "k_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100]\n",
    "\n",
    "X_train_folds = []\n",
    "y_train_folds = []\n",
    "\n",
    "# Split up the data into folds\n",
    "# X_train_folds and y_train_folds lits of length num_folds\n",
    "\n",
    "X_train_folds = np.array_split(X_train3, num_folds)\n",
    "y_train_folds = np.array_split(y_train3, num_folds)\n",
    "\n",
    "\n",
    "# A dictionary of length num_folds holding the accuracies for different values of k \n",
    "k_to_accuracies = {}\n",
    "\n",
    "# Ataovy ary ilay k-fold cross validation \n",
    "# Atao ao anaty k_to_accuracies ny accuracy isaky ny valeur k\n",
    "\n",
    "## We will try successivly all the hyper-parameter\n",
    "for k in k_choices :\n",
    "    model = KNearestNeighborClassifier(k)\n",
    "    k_to_accuracies[k] = []\n",
    "    ## We will now train or/and test\n",
    "    for i in range(0, num_folds) :\n",
    "        ## define our test\n",
    "        Xtest = X_train_folds[i]\n",
    "        ytest = y_train_folds[i]\n",
    "        ## define our train (matrix) by removing the test\n",
    "        Xtrain = np.concatenate(np.delete(X_train_folds, i, axis=0))\n",
    "        ytrain = np.concatenate(np.delete(y_train_folds, i, axis=0))\n",
    "        ## train and predict, accuracy\n",
    "        model.fit(Xtrain, ytrain)\n",
    "        model_accuracy = accuracy_score(ytest, model.predict(Xtest))\n",
    "        \n",
    "        ## adding it to our dictionnary\n",
    "        k_to_accuracies[k].append(model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "110ef5f7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f539a48494772e41230a2b590897508",
     "grade": true,
     "grade_id": "cell-4d0c3c0a08ed3f82",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1, accuracy = 0.991701\n",
      "k = 1, accuracy = 0.983402\n",
      "k = 1, accuracy = 0.983402\n",
      "k = 1, accuracy = 0.970833\n",
      "k = 1, accuracy = 0.995833\n",
      "k = 3, accuracy = 0.987552\n",
      "k = 3, accuracy = 0.983402\n",
      "k = 3, accuracy = 0.987552\n",
      "k = 3, accuracy = 0.970833\n",
      "k = 3, accuracy = 0.983333\n",
      "k = 5, accuracy = 0.991701\n",
      "k = 5, accuracy = 0.983402\n",
      "k = 5, accuracy = 0.987552\n",
      "k = 5, accuracy = 0.975000\n",
      "k = 5, accuracy = 0.987500\n",
      "k = 8, accuracy = 0.979253\n",
      "k = 8, accuracy = 0.970954\n",
      "k = 8, accuracy = 0.987552\n",
      "k = 8, accuracy = 0.966667\n",
      "k = 8, accuracy = 0.983333\n",
      "k = 10, accuracy = 0.983402\n",
      "k = 10, accuracy = 0.970954\n",
      "k = 10, accuracy = 0.979253\n",
      "k = 10, accuracy = 0.966667\n",
      "k = 10, accuracy = 0.975000\n",
      "k = 12, accuracy = 0.979253\n",
      "k = 12, accuracy = 0.975104\n",
      "k = 12, accuracy = 0.979253\n",
      "k = 12, accuracy = 0.958333\n",
      "k = 12, accuracy = 0.979167\n",
      "k = 15, accuracy = 0.983402\n",
      "k = 15, accuracy = 0.962656\n",
      "k = 15, accuracy = 0.983402\n",
      "k = 15, accuracy = 0.966667\n",
      "k = 15, accuracy = 0.958333\n",
      "k = 20, accuracy = 0.970954\n",
      "k = 20, accuracy = 0.958506\n",
      "k = 20, accuracy = 0.975104\n",
      "k = 20, accuracy = 0.958333\n",
      "k = 20, accuracy = 0.954167\n",
      "k = 50, accuracy = 0.946058\n",
      "k = 50, accuracy = 0.925311\n",
      "k = 50, accuracy = 0.962656\n",
      "k = 50, accuracy = 0.920833\n",
      "k = 50, accuracy = 0.925000\n",
      "k = 100, accuracy = 0.904564\n",
      "k = 100, accuracy = 0.896266\n",
      "k = 100, accuracy = 0.937759\n",
      "k = 100, accuracy = 0.891667\n",
      "k = 100, accuracy = 0.875000\n"
     ]
    }
   ],
   "source": [
    "for k in sorted(k_to_accuracies):\n",
    "    for accuracy in k_to_accuracies[k]:\n",
    "        print('k = %d, accuracy = %f' % (k, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cf61ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
